slug: nomad-monitoring
id: mxfbvvo8n5l2
type: track
title: Nomad Monitoring
teaser: Monitor a Nomad cluster and jobs with Prometheus.
description: |-
  The Nomad client and server agents collect runtime [telemetry](https://www.nomadproject.io/docs/telemetry/index.html). Operators can use this data to gain real-time visibility into their Nomad clusters and improve performance. Additionally, Nomad operators can set up monitoring and alerting against these metrics and export the metrics to tools like Prometheus, Grafana, Graphite, DataDog, and Circonus.

  This track will guide you through implementing the [Using Prometheus to Monitor Nomad Metrics](https://www.nomadproject.io/guides/operations/monitoring-and-alerting/prometheus-metrics.html) guide.

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics), [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster), and [Nomad Multi-Server Cluster](https://instruqt.com/hashicorp/tracks/nomad-multi-server-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- Nomad
- cluster
- monitoring
- telemetry
- Prometheus
owner: hashicorp
developers: []
private: true
published: true
challenges:
- slug: fabio-and-prometheus-jobs
  id: m4ly1zb0xyjh
  type: challenge
  title: Deploy Fabio and Prometheus
  teaser: Run Nomad jobs that deploy Fabio and Prometheus to the Nomad cluster.
  assignment: |-
    In this challenge, you will run jobs that deploy Fabio and Prometheus to the Nomad cluster.

    That's right: We'll be monitoring Nomad with an application (Prometheus) deployed by Nomad to the Nomad cluster!

    First check that the Nomad and Consul agents were deployed properly by running these commands on the "Server" tab:
    ```
    consul members
    nomad server members
    nomad node status
    ```

    The first should show 4 Consul agents, the second should show 1 Nomad server, and the third should show 3 Nomad clients.

    ## Run the Fabio Job
    The job file for Fabio has been created for you.
    - View the file called `fabio.nomad` from "Config" tab.
    - Register the `fabio` job from the "Server" tab.
    ```
    cd nomad
    nomad job run fabio.nomad
    ```

    - Click on "Fabio" tab. This connects to Nomad client on port `9998` for the fabio web UI.
    - You can connect to any client, but we prepared `client-1` as a tab.
    - The Fabio routing table will be empty. We have not deployed anything that fabio can route to, yet.

    ## Run the Prometheus Job
    NOTE: The setup script for the server has deployed two versions of the `prometheus.nomad` job specification file to the `/root/nomad` directory, `prometheus1.nomad` and `prometheus2.nomad`. Both files specify the job name as `prometheus`. You should run `nomad job run `prometheus1.nomad` in this step.

    The job file for Prometheus has been created for you.
    - View the file called `prometheus1.nomad` from "Config" tab.

    Register the `prometheus` job from the "Server" tab.
    ```
    nomad job run prometheus1.nomad
    ```

    Verify that fabio and prometheus are running on "three" and "one" clients, respectively.
    ```shell
    nomad node status
    nomad job status fabio | grep -A 5 Allocations
    nomad job status prometheus | grep -A 5 Allocations
    ```

    - Connect to any of the Nomad clients on port `9999` (tabs Prometheus UI 1, 2 or 3).
    - NOTE: There is only one instance of Prometheus, but fabio routes you to the correct node.
    - You might need to click the refresh icon (top right).

    Use Prometheus to query how many jobs are running on our Nomad cluster from "Prometheus UI" tab.
    - Type `nomad_nomad_job_summary_running` into the query section.
    - Click "Execute".
    ![running-jobs](https://www.nomadproject.io/assets/images/running-jobs-564b55df.png)
    - The value of our `fabio` job is `3` since it is using the [system](https://www.nomadproject.io/docs/schedulers.html#system) scheduler type. We are running three Nomad clients in our demo cluster.
    - The value of our `prometheus` job is `1` since we only deployed one instance of it.
    - To see the description of other metrics, visit the [telemetry](https://www.nomadproject.io/docs/configuration/telemetry.html) section.
  notes:
  - type: text
    contents: |-
      The Nomad client and server agents collect runtime [telemetry](https://www.nomadproject.io/docs/telemetry/index.html). Operators can use this data to gain real-time visibility into their Nomad clusters and improve performance.

      Additionally, Nomad operators can set up monitoring and alerting against these metrics and export the metrics to tools like Prometheus, Grafana, Graphite, DataDog, and Circonus.

      This track will guide you through implementing the [Using Prometheus to Monitor Nomad Metrics](https://www.nomadproject.io/guides/operations/monitoring-and-alerting/prometheus-metrics.html) guide.
  - type: text
    contents: |-
      The setup scripts for this track have configured 4 VMs running Nomad and Consul agents. One is functioning as a server while the other 3 are functioning as clients, both with regard to Nomad and to Consul.

      We've put all the server and client configuration files and job specification files on the nomad-server VM so that you can conveniently see them in a single Instruqt tab.
  - type: text
    contents: |-
      In this challenge, you will first run a job that deploys the reverse proxy server [Fabio](https://fabiolb.net) that uses [Consul](https://www.consul.io).

      You will then run a job that deploys [Prometheus](https://prometheus.io/docs/introduction/overview), an open-source systems monitoring and alerting tool.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Client 3
    type: terminal
    hostname: nomad-client-3
  - title: Nomad UI
    type: service
    hostname: nomad-server
    port: 4646
  - title: Fabio UI
    type: service
    hostname: nomad-client-1
    port: 9998
  - title: Prometheus1
    type: service
    hostname: nomad-client-1
    port: 9999
  - title: Prometheus2
    type: service
    hostname: nomad-client-2
    port: 9999
  - title: Prometheus3
    type: service
    hostname: nomad-client-3
    port: 9999
  difficulty: basic
  timelimit: 900
- slug: add-alertmanager
  id: aaqomh65hrcz
  type: challenge
  title: Deploy Alertmanager
  teaser: Deploy Alertmanager] to the Nomad cluster and connect Prometheus to it.
  assignment: |-
    In this challenge, you will deploy the Prometheus Alertmanager to the Nomad cluster and modify the `prometheus.nomad` job to integrate the Prometheus server with Alertmanager.

    NOTE: Prometheus sends alerts to Alertmanager. Alertmanager sends out notifications to designated receivers.

    ## Run the Alertmanager Job
    The `alertmanager` job has already been created for you.
    - View the file called `alertmanager.nomad` from "Config Files" tab.
    - Run the `alertmanager` job from the "Server" tab.
    ```shell
    cd /root/nomad
    nomad job run alertmanager.nomad
    ```
    - Check the status
    ```
    nomad job status alertmanager
    ```

    ## Modify and Re-run the Prometheus Job
    NOTE: The setup script for the server has deployed two version of the `prometheus.nomad` job specification file to the `/root/nomad` directory, `prometheus1.nomad` and `prometheus2.nomad`. Both files specify the job name as `prometheus`. You should run `nomad job run prometheus2.nomad` in this step. You will not need
    to edit either file.

    The modified prometheus job has already been created for you.
    - View the file called `prometheus2.nomad` from "Config Files" tab.

    NOTE: We added a few important sections to this job file:
    - added another template stanza that defines an alerting rule for our web server. Prometheus will send out an alert if it detects the `webserver` service has disappeared.
    - added an `alerting` block to our Prometheus configuration as well as a `rule_files` block to make Prometheus aware of Alertmanager as well as the rule we have defined.
    - are also scraping Alertmanager along with our web server.

    - Run the modified job from the "Server" tab.
    ```shell
    cd /root/nomad
    nomad job run prometheus2.nomad
    ```
    - Check the status
    ```
    nomad job status prometheus
    ```
    - You should see the following allocations and status: one allocation `running` and at least one allocation `complete`.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy the Prometheus [Alertmanager](https://prometheus.io/docs/alerting/alertmanager) to the Nomad cluster. It manages alerts.

      You will then modify the `prometheus.nomad` job to integrate the Prometheus server with Alertmanager and re-run the Prometheus job.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Client 3
    type: terminal
    hostname: nomad-client-3
  - title: Nomad UI
    type: service
    hostname: nomad-server
    port: 4646
  difficulty: basic
  timelimit: 900
- slug: add-web-server
  id: wzwlz6v21qqr
  type: challenge
  title: Deploy a Web Server
  teaser: Deploy a web server to the Nomad cluster.
  assignment: |-
    In this challenge, you will deploy a web server to the Nomad cluster by running the `webserver.nomad` job.

    ## Run the Web Server Job
    The `webserver` job has already been created for you.
    - View the file called `webserver.nomad` from "Config Files" tab.
    - Run the `webserver` job from the "Server" tab.
    ```shell
    cd /root/nomad
    nomad job run webserver.nomad
    ```
    - Check the status
    ```
    nomad job status webserver
    ```
    At this point, re-run your Prometheus job.

    - Go to "Prometheus" tab. Click Status -> Targets.
    - After a few seconds, you will see the `webserver` and `alertmanager` appear in your list of targets.
    ![](https://www.nomadproject.io/assets/images/new-targets-7e2bcd93.png)

    - Go to the "Alerts" section of the Prometheus web interface.
    - Note the alert that we have configured. No alerts are active because our web server is up and running.
    ![](https://www.nomadproject.io/assets/images/alerts-ee875c5e.png)

    ## Stop the Web Server
    - Run `nomad stop webserver` in "Server" tab to stop our webserver.
    - Go to "Prometheus" tab. Click "Alerts".
    - Note that we have an active alert.
    ![Active Alerts](https://www.nomadproject.io/assets/images/active-alert-cfcc7e45.png)

    Verify that Alertmanager has received this alert as well.
    - Go to the "Alertmanager" web interface from "Alertmanager" tab.
    - NOTE: Alertmanager has been configured behind fabio. You can go to the IP address of any of your client nodes at port `9999` and use `/alertmanager` as the route.
    - For example: `<client_node_IP>:9999/alertmanager`
    - You should see that Alertmanager has received the alert.
    ![Alertmanager Web UI](https://www.nomadproject.io/assets/images/alertmanager-webui-612ce20c.png)

    ## Start the Web Server (optional)
    - Run `cd /root/nomad && nomad job start webserver.nomad` in "Server" tab to stop our webserver.
    - Go to "Prometheus" tab. Click "Alerts".
    - Note that we do not have an active alert.

    Verify that Alertmanager has removed this alert as well.
    - Go to the "Alertmanager" web interface from "Alertmanager" tab.
    - You should see that Alertmanager has removed the alert.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy a web server to the Nomad cluster.

      You will then stop the web server and see alerts generated in Prometheus and Alertmanager from the metrics that Nomad is sending them.
  tabs:
  - title: Config Files
    type: code
    hostname: nomad-server
    path: /root/nomad/
  - title: Server
    type: terminal
    hostname: nomad-server
  - title: Client 1
    type: terminal
    hostname: nomad-client-1
  - title: Client 2
    type: terminal
    hostname: nomad-client-2
  - title: Client 3
    type: terminal
    hostname: nomad-client-3
  - title: Nomad UI
    type: service
    hostname: nomad-server
    port: 4646
  - title: Fabio UI
    type: service
    hostname: nomad-client-1
    port: 9998
  - title: Prometheus UI
    type: service
    hostname: nomad-client-1
    port: 9999
  - title: Alertmanager
    type: service
    hostname: nomad-client-1
    path: /alertmanager
    port: 9999
  difficulty: basic
  timelimit: 900
checksum: "8868373877923465657"
