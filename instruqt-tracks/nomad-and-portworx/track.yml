slug: nomad-and-portworx
id: h7wajhkey5qc
type: track
title: Nomad Integration With Portworx
teaser: |
  Learn how Nomad and Portworx support stateful workloads deployed by Nomad jobs.
description: |-
  Some Nomad workloads need to persist data that will still be available if the job that runs the workloads are stopped and restarted.

  Nomad supports stateful workloads with 3 different options:
    * [Nomad Host Volumes](https://nomadproject.io/docs/configuration/client/#host_volume-stanza) that are managed by Nomad and can be used with any Nomad task driver
    * [Docker Volume Drivers](https://docs.docker.com/engine/extend/plugins_volume/#create-a-volumedriver) such as Portworx that are externally managed and can only be used with the Docker task driver
    * [CSI Plugins](https://github.com/container-storage-interface/spec/blob/master/spec.md) that are also externally managed but can be used with any Nomad task driver.

  This track will guide you through using a [Docker Volume Driver](https://nomadproject.io/docs/drivers/docker/#inlinecode-volumes-16) and [Portworx](https://docs.portworx.com/install-with-other/nomad) to persist data for an HA MySQL database deployed by Nomad. It is based on the [Stateful Workloads with Portworx](https://learn.hashicorp.com/nomad/stateful-workloads/portworx) guide.

  Before running this track, we suggest you run the [Nomad Basics](https://play.instruqt.com/hashicorp/tracks/nomad-basics) track.
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- Nomad
- stateful
- Portworx
- storage
owner: hashicorp
developers:
- roger@hashicorp.com
- tharris@hashicorp.com
private: true
published: false
challenges:
- slug: verify-nomad-cluster-health
  id: ap4o4x7mtxn9
  type: challenge
  title: Verify the Health of Your Nomad Cluster
  teaser: |
    Verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts.
  assignment: |-
    In this challenge, you will verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 3 Nomad/Consul clients with Nomad 0.11.0 and Consul 1.7.2. These VMs have been created in a GCP project within this Instruqt track.

    First, determine the public IP of your Nomad server by running this command on the "Cloud CLI" tab:
    ```
    echo $nomad_server_ip
    ```

    You can visit the Nomad UI in any browser tab outside the Instruqt tab with the URL, `http://<NOMAD_IP>:4646` where <NOMAD_IP\> is the value of $nomad_server_ip.

    You can visit the Consul UI in any browser tab outside the Instruqt tab with the URL, `http://<NOMAD_IP>:8500` where <NOMAD_IP\> is the value of $nomad_server_ip.

    Next, SSH to the Nomad server with this command:
    ```
    gcloud compute ssh nomad-server-1 --zone europe-west1-b  --project $INSTRUQT_GCP_PROJECT_NOMAD_PROJECT_ID --strict-host-key-checking no
    ```

    Now, verify that all 4 Consul agents are running and connected to the cluster by running this command on the "Cloud CLI" tab:
    ```
    consul members
    ```
    You should see 4 Consul agents with the "alive" status including 1 server and 3 clients.

    Check that the Nomad server is running by running this command on the "Cloud CLI" tab:
    ```
    nomad server members
    ```
    You should see 1 Nomad server with the "alive" status.

    Check the status of the Nomad client nodes by running this command on the "Cloud CLI" tab:
    ```
    nomad node status
    ```
    You should see 3 Nomad clients with the "ready" status.

    You can also check the status of the Nomad server and clients in the Nomad and Consul UIs.

    In the next challenge, you will run a Nomad job that will install Portworx on all 3 Nomad clients.
  notes:
  - type: text
    contents: |-
      Nomad supports stateful workloads with 3 different options:
        * [Nomad Host Volumes](https://nomadproject.io/docs/configuration/client/#host_volume-stanza) that are managed by Nomad and can be used with any Nomad task driver
        * [Docker Volume Drivers](https://docs.docker.com/engine/extend/plugins_volume/#create-a-volumedriver) such as Portworx that are externally managed and can only be used with the Docker task driver
        * [CSI Plugins](https://github.com/container-storage-interface/spec/blob/master/spec.md) that are also externally managed but can be used with any Nomad task driver.

      In this track, you will use a Docker volume driver and [Portworx](https://docs.portworx.com/install-with-other/nomad) to persist data for an HA MySQL database.
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

      In later challenges, you will do <SOME THINGS\>
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Links
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 7200
- slug: install-portworx
  id: jndu9wxvuyj6
  type: challenge
  title: Install Portworx on the Nomad Clients
  teaser: |
    Install Portworx on the Nomad clients.
  assignment: |-
    In this challenge, you will deploy Portworx on all 3 Nomad clients.

    In the next challenge, you will do <SOMETHING\>.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy Portworx on all 3 Nomad clients.

      In the next challenge, you will do <SOME THINGS\>.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Links
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 3600
checksum: "16403737712222275309"
