slug: nomad-and-csi-plugins
id: 8gcf7uizgmdf
type: track
title: Nomad CSI Plugins
teaser: |
  Learn how Nomad CSI plugins support stateful workloads.
description: |-
  Some Nomad workloads need to persist data that will still be available if the job that runs the workloads are stopped and restarted.

  Nomad supports stateful workloads with 3 different options:
    * [Nomad Host Volumes](https://nomadproject.io/docs/configuration/client/#host_volume-stanza) that are managed by Nomad and can be used with any Nomad task driver
    * [Docker Volume Drivers](https://docs.docker.com/engine/extend/plugins_volume/#create-a-volumedriver) such as Portworx that are externally managed and can only be used with the Docker task driver
    * [CSI Plugins](https://github.com/container-storage-interface/spec/blob/master/spec.md) that are also externally managed but can be used with any Nomad task driver.

  This track will guide you through using Nomad CSI Plugins to persist data for a MySQL database using a Google Persitent Disk and the GCP Driver. It is based on the [Stateful Workloads with Container Storage Interface](https://learn.hashicorp.com/nomad/stateful-workloads/csi-volumes) guide.

  Before running this track, we suggest you run the [Nomad Basics](https://play.instruqt.com/hashicorp/tracks/nomad-basics) track.

  There are other tracks related to different Nomad storage options:

  Host Volumes: https://play.instruqt.com/hashicorp/tracks/nomad-host-volumes
  Portworx: https://play.instruqt.com/hashicorp/tracks/nomad-and-portworx
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- Nomad
- stateful
- CSI plugins
- storage
owner: hashicorp
developers:
- roger@hashicorp.com
- tharris@hashicorp.com
private: true
published: false
challenges:
- slug: verify-nomad-cluster-health
  id: 0e1x2hnlvsaf
  type: challenge
  title: Verify the Health of Your Nomad Cluster
  teaser: |
    Verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts.
  assignment: "In this challenge, you will verify the health of the Nomad cluster
    that has been deployed for you by the track's setup scripts. This will include
    checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 3 Nomad/Consul clients with Nomad
    0.11.1 and Consul 1.7.2.

    You can visit the Google Cloud Console for the GCP project associated with this lab by selecting the 'Cloud Links' tab, right-clicking
    the Project ID for the \"nomad\" Google cloud project, and then opening the console
    in a new tab or window. If you already have your own GCP account, it will be easier
    to use an incognito window.

    First, determine the public IP of your Nomad server by running
    this command on the \"Cloud CLI\" tab:\n```\necho $nomad_server_ip\n```\nYou can
    visit the Nomad UI in any browser tab outside the Instruqt tab with the URL, `http://<NOMAD_IP>:4646`
    where <NOMAD_IP\\> is the value of $nomad_server_ip.\nYou can visit the Consul
    UI in any browser tab outside the Instruqt tab with the URL, `http://<NOMAD_IP>:8500`
    where <NOMAD_IP\\> is the value of $nomad_server_ip.\nNext, SSH to the Nomad server
    with this command:\n```\ngcloud compute ssh nomad-server-1 --zone europe-west1-b
    \ --project $INSTRUQT_GCP_PROJECT_NOMAD_PROJECT_ID --strict-host-key-checking
    no\n```\nNow, verify that all 4 Consul agents are running and connected to the
    cluster by running this command on the \"Cloud CLI\" tab:\n```\nconsul members\n```\nYou
    should see 4 Consul agents with the \"alive\" status including 1 server and 3
    clients.\nCheck that the Nomad server is running by running this command on the
    \"Cloud CLI\" tab:\n```\nnomad server members\n```\nYou should see 1 Nomad server
    with the \"alive\" status.\nCheck the status of the Nomad client nodes by running
    this command on the \"Cloud CLI\" tab:\n```\nnomad node status\n```\nYou should
    see 3 Nomad clients with the \"ready\" status."
  notes:
  - type: text
    contents: |-
      Nomad supports stateful workloads with 3 different options:
        * [Nomad Host Volumes](https://nomadproject.io/docs/configuration/client/#host_volume-stanza) that are managed by Nomad and can be used with any Nomad task driver
        * [Docker Volume Drivers](https://docs.docker.com/engine/extend/plugins_volume/#create-a-volumedriver) such as Portworx that are externally managed and can only be used with the Docker task driver
        * [CSI Plugins](https://github.com/container-storage-interface/spec/blob/master/spec.md) that are also externally managed but can be used with any Nomad task driver.
      In this track, you will use the GCE Persistent Disk driver to persist data for MySQL database.
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.
      In later challenges you will run Nomad jobs that deploy the Google Persistent Disk CSI driver, deploy a MySQL database, and verify that data written to MySQL survives stopping and re-running the MySQL job.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Cloud Links
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 7200
- slug: deploy-gcepd-driver
  id: dwnjvrvf1vhr
  type: challenge
  title: Deploy CSI Plugin as Nomad Job
  teaser: Deploy the CSI plugin
  assignment: |-
    The GCE-PD CSI driver is deployed in Nomad, There are two jobs to run. One is the “controller” job and one is the “nodes” job.
    Once these are running, the GCE-PD driver will register as a plugin within Nomad.

    The "controller" job runs on a Nomad client and has the function of being an endpoint for requests for new volume registrations, maintaining the state of the registered volumes etc.
    The "nodes" job is run on every nomad client and is responsible for mapping and unmapping storage devices.
    The Google Compute Engine Persistent Disk (gcepd) driver is deployed as a nomad job using the docker driver.

    The gcepd docker container needs credentials to interact with GCE resources.
    This is achieved by creating a service account, creating a key for the service account and then passing the service account key to the gcepd container.
    Service account credentials have been created as part of the this lab's setup.

    The containers access the key by reading an environment variable named ```GOOGLE_APPLICATION_CREDENTIALS``` within the container runtime.

    There are multiple ways to set sensitive environment variables for a Nomad job, but this example uses Consul templates.

    The Nomad job will query Consul’s key-value store for the ```service_account``` key and export the value as an environment variable named ```GOOGLE_APPLICATION_CREDENTIALS``` within the container.

    issue these comands in the "Nomad Server" tab.

    SSH into the Nomad Server:
    ```
    gcloud compute ssh nomad-server-1 --zone europe-west1-b
    ```
    Write the credentials to Consul's key-value store:
    ```
    consul kv put service_account @creds.json
    ```
    now you need to run the controller job which is located in the nomad-csi folder. Before you do, examine the contents of the file. Note the "csi_plugin" stanza, which allows the task to specify it provides a Container Storage Interface plugin to the cluster. Nomad will automatically register the plugin so that it can be used by other jobs to claim volumes. Examine the file:
    ```
    cat nomad-csi/controller.nomad
    ```
    Run the job:
    ```
    nomad run nomad-csi/controller.nomad
    ```
    Now run the nodes job:
    ```
    nomad run nomad-csi/nodes.nomad
    ```
    Check the status of the jobs to ensure they are in a running state:
    ```
    nomad job status controller
    ```
    ```
    nomad job status nodes
    ```
    Check the status of the plugin to ensure it is registered. Please note, it usually takes around 30 seconds to register the CSI plugin with Nomad. run this command:
    ```
    nomad plugin status
    ```
    Please exit the SSH session (this will enable the "Check" to work correctly)

    ```
    exit
    ```
  notes:
  - type: text
    contents: Deploy the CSI plugin as a Nomad Job
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Nomad Server
    type: terminal
    hostname: nomad-server
  - title: Cloud Links
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 7200
- slug: create-persistent-disk
  id: wsoiwyeiglqn
  type: challenge
  title: Create a Google Persistent Disk
  teaser: -|
    Create a Google Persistent Disk for the CSI plugin to mount
  assignment: |-

    You are now ready to create a persistent disk and regiser the disk as a Nomad CSI volume. In the 'Cloud CLI' tab, enter the following gcloud commands:

    ```
    gcloud mcompute disks create mysql --size 10 --zone europe-west1-b
    ```
    Describe the details of the disk, we will need the ```self_link``` value in the next step:
    ```
    gcloud compute disks describe mysql --zone europe-west1-b
    ```
    SSH into the Nomad Server:
    ```
    gcloud compute ssh nomad-server-1 --zone europe-west1-b
    ```
    Now open the volume.hcl file on the Nomad Server tab (which is in the nomad-csi folder) and edit the ```external_id``` field with the value of the self_link from the previous step.  \nplease only copy
    from ```projects/``` onwards, the ```https://googleapis.com``` is not needed.

    Close and save the file:
    ```
    vim nomad-csi/volume.hcl
    ```
    Register the volume:
    ```
    nomad volume register nomad-csi/volume.hcl
    ```
    Check the volume is registered. You should see the mysql volume in the output:

    ```
    nomad volume status
    ```
    Please exit the SSH session - this will enable the "Check" to work correctly.

    ```
    exit
    ```
  notes:
  - type: text
    contents: Create a Google Persistent disk and register with nomad as a CSI volume.
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Nomad Server
    type: terminal
    hostname: nomad-server
  - title: Cloud Links
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 7200
- slug: deploy-mysql
  id: pxhpjsbuxe5a
  type: challenge
  title: Deploy MySQL
  teaser: |
    Deploy a MySQL database that uses a Nomad CSI Volume for storage that is provided by the underlying GCE persistent disk
  assignment: |-
    You are now ready to deploy a MySQL database with a Nomad job file.
    the MySQL database will be able to persist its data as the Nomad job attaches the CSI volume you previously registered with Nomad.

    SSH into the Nomad Server
    ```
    gcloud compute ssh nomad-server-1 --zone europe-west1-b
    ```
    Run the mysql job:
    ```
    nomad job run nomad-csi/mysql.nomad
    ```
    Check that the job is in a running state:
    ```
    nomad job status mysql
    ```
    Using the mysql client, connect to the database to access the MySQL table (the password for this demo is ```password```):
    ```
    mysql -h mysql-server.service.consul -u web -p -D itemcollection
    ```
    Display the contents of the items table:
    ```
    select * from items;
    ```
    Now add some data to the table:
    ```
    INSERT INTO items (name) VALUES ('glove');
    ```
    Run INSERT INTO as many times as you wish with different values:
    ```
    INSERT INTO items (name) VALUES ('dog');
    ```
    Once you have added values to the table, return back to the CLI:
    ```
    exit
    ```
  notes:
  - type: text
    contents: Deploy MySQL as a Nomad Job and write data to a table
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Nomad Server
    type: terminal
    hostname: nomad-server
  - title: Cloud Links
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 7200
- slug: destroy-redeploy
  id: 8axvovofeilq
  type: challenge
  title: Destroy and re-deploy the job and observe the data has persisted
  teaser: |
    Destroy the MySQL job and then re-deploy. The data is still there, like magic.
  assignment: |-
    SSH into the Nomad Server
    ```
    gcloud compute ssh nomad-server-1 --zone europe-west1-b
    ```
    You are now ready to destroy the database.run the following command to stop and purge the job from the cluster:
    ```
    nomad stop -purge mysql-server
    ```
    Verify mysql is no longer running in the cluster:
    ```
    nomad job status mysql
    ```
    Using the same mysql.nomad job file, re-deploy the database:
    ```
    nomad run nomad-csi/mysql.nomad
    ```
    Connect to the database:
    ```
    mysql -h mysql-server.service.consul -u web -p -D itemcollection
    ```
    Once you re-connect to MySQL, you should be able to verify that the information you added prior to destroying the database is still present:
    ```
    select * from items;
    ```
    You have successfully completed the track. Feel free to play around in the environment!
  notes:
  - type: text
    contents: Stop and purge the MySQL job and then redeploy. See your persistent
      data!
  tabs:
  - title: Cloud CLI
    type: terminal
    hostname: cloud-client
  - title: Nomad Server
    type: terminal
    hostname: nomad-server
  - title: Cloud Links
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 7200
checksum: "1108091270258918541"
