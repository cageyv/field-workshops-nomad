slug: nomad-update-strategies
id: ijfbyi1drwpu
type: track
title: Nomad Job Update Strategies
teaser: |
  Explore Nomad job update strategies including rolling, blue/green, and canary updates.
description: |-
  Most applications are long-lived and require updates over time. Whether you are deploying a new version of your web application or updating to a new version of a database, Nomad has built-in support for rolling, blue/green, and canary updates.

  This track will guide you through implementing these update strategies based on the [Nomad Job Update Strategies](https://learn.hashicorp.com/nomad/update-strategies) guide.

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics), [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster), and [Nomad Multi-Server Cluster](https://instruqt.com/hashicorp/tracks/nomad-multi-server-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png
tags:
- Nomad
- updates
- rolling
- blue/green
- canary
owner: hashicorp
developers:
- roger@hashicorp.com
private: true
published: true
challenges:
- slug: verify-nomad-cluster-health
  id: ehjpxbybs49x
  type: challenge
  title: Verify the Health of Your Nomad Enterprise Cluster
  teaser: |
    Verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts.
  assignment: |-
    In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 3 Nomad/Consul clients. The [Consul Connect integration](https://nomadproject.io/guides/integrations/consul-connect) has been enabled on the server and clients since the chatapp uses it to talk to the MongoDB database. Additionally, a Nomad [host volume](https://nomadproject.io/docs/configuration/client/#host_volume-stanza) called "mongodb_mount" has been configured on the clients under the /opt/mongodb/data directory so that MongoDB can persist its data.

    First, verify that all 4 Consul agents are running and connected to the cluster by running this command on the "Server" tab:
    ```
    consul members
    ```
    You should see 4 Consul agents with the "alive" status.

    Check that the Nomad server is running by running this command on the "Server" tab:
    ```
    nomad server members
    ```
    You should see 1 Nomad server with the "alive" status.

    Check the status of the Nomad client nodes by running this command on the "Server" tab:
    ```
    nomad node status
    ```
    You should see 3 Nomad clients with the "ready" status.

    You can also check the status of the Nomad server and clients in the Nomad and Consul UIs.

    In the next challenge, you will configure Nomad Enterprise namespaces and resource quotas.
  notes:
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

      In later challenges, you will run chatapp and mongodb jobs and update the first using rolling, blue/green, and canary update strategies.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 300
- slug: deploy-the-jobs
  id: tglnvylrrmeo
  type: challenge
  title: Deploy the Chat and MongoDB Jobs
  teaser: |
    Deploy the initial versions of the Chat and MongoDB jobs.
  assignment: |-
    In this challenge, you will deploy the inital versions of the Chat and MongoDB jobs.

    Inspect the "mongodb.nomad" job specification file on the "Jobs" tab. This will deploy a "db" task group that runs the MongoDB database on port 27017 from the "mongo" Docker image.

    Note that the database persists its data to the Nomad volume "mongodb_vol" which uses the "mongodb_mount" host volume that was configured on each Nomad client. The "mongodb_vol" volume is mounted inside the Docker container on the path "/data/db".

    Navigate to the /root/nomad/jobs directory on the "Server" tab with this command:
    ```
    cd /root/nomad/jobs
    ```

    Run the "mongodb.nomad" job with this command on the "Server" tab:
    ```
    nomad job run mongodb.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "d66b5ced"
    Evaluation triggered by job "mongodb"
    Evaluation within deployment: "d5d97844"
    Allocation "3c257515" created: node "8a0ecd90", group "db"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "d66b5ced" finished with status "complete"
    `<br>

    You can check the job in the Nomad UI after waiting 15-30 seconds, clicking the Instruqt refresh button above the Nomad UI, and then selecting the "mongodb" job. You might need to make your browser window a bit wider to see the UI properly and so that the Instruqt buttons do not overlap the Nomad UI tab. You could also click the rectangular button next to the refresh button to hide this assignment. After no more than 1 minute, you should see that the job has 1 healthy allocation.

    Alternatively, you can run the `nomad job status mongodb` command on the "Server" tab to check the status of the job and validate that the "db" task group is healthy.

    Next, inspect the "chatapp.nomad" job specification file on the "Jobs" tab. This job deploys 3 instances of a "chat-app" task group that runs a custom Docker image, "lhaig/anon-app:0.02" created by two HashiCorp solutions engineers, Guy Barrows and Lance Haig. Note that the job specifies the "0.02" tag. Later, when we update the app in various ways, we will specify a different tag to use a different version of it.

    The "chat-app" task group runs on the static port 9002 on the Nomad clients; that port is mapped to port 5000 inside the Docker containers.

    Additionally, the task group has the following stanza:<br>
    `
    update {
      max_parallel     = 1
      health_check     = "task_states"
      min_healthy_time = "15s"
      healthy_deadline = "2m"
    }
    `<br>

    This tells Nomad that it should update one instance of the task group at a time, base the health of each instance on the state of its tasks, require a new allocation to be healthy for 15 seconds before marking it as healthy, and require a new allocation to be healthy after no more than 2 minutes; if an allocation is not healthy within 2 minutes, it will be marked as unhealthy.

    Finally, the "chat-app" task group uses Consul Connect [sidecar proxies](https://www.consul.io/docs/connect/proxies.html) to talk to the MongoDB database using mutual Transport Layer Security (mTLS) certificates. This is implemented by this stanza:<br>
    `
    connect {
      sidecar_service {
        tags = ["chat-app-proxy"]
        proxy {
          upstreams {
            destination_name = "mongodb"
            local_bind_port = 27017
          }
        }
      }
    } # end connnect
    `<br>

    Run the "chatapp.nomad" job on the "Server" tab with this command:
    ```
    nomad job run chatapp.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "83924539"
    Evaluation triggered by job "chat-app"
    Evaluation within deployment: "e7cde396"
    Allocation "5fa9f018" created: node "48f9e81c", group "chat-app"
    Allocation "712e8d99" created: node "8a0ecd90", group "chat-app"
    Allocation "c81e3ab3" created: node "acba5ef2", group "chat-app"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "83924539" finished with status "complete" "complete"
    `<br>

    After waiting about 30 seconds, check the status of the "chat-app" job with this command on the "Server" tab:
    ```
    nomad job status chat-app
    ```

    This should indicate that the job is running and have an Allocations section at the bottom that looks like this:<br>
    `
    Allocations
    ID        Node ID   Task Group  Version  Desired  Status   Created  Modified
    8b883f54  8a0ecd90  chat-app    0        run      running  21s ago  18s ago
    c9cc1c1c  acba5ef2  chat-app    0        run      running  21s ago  14s ago
    ca660509  48f9e81c  chat-app    0        run      running  21s ago  14s ago
    `<br>

    Note that the Node IDs of the 3 allocations are all different. In other words, Nomad scheduled one instance of the chat-app task to each Nomad client. It had to do this because we used the static port 9002 which would prevent more than one instance of the app running on any of the Nomad clients.

    You can also check out the "chat-app" job in the Nomad UI. Within 1 minute of running the job, all 3 allocations should be healthy.

    You can also verify that the Nomad task groups were automatically regisâ€ ered as Consul services by looking at the "Services" tab of the Consul UI. Note the sidecar proxy services created by Nomad's integration with Consul Connect.

    You can visit the chat-app UI on the "Chat 1", "Chat 2", and "Chat 3" tabs. You can send messages and then see them show up inside the chat-app UI on the other Chat tabs, but only after selecting those tabs and clicking the Instruqt refresh button.

    In the next challenge, you will update the Chat app using the rolling update strategy.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy the inital versions of the Chat and MongoDB jobs.

      In the next challenge, you will update the Chat app using the rolling update strategy.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Fabio UI
    type: service
    hostname: nomad-client-1
    port: 9998
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 9002
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 9002
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 9002
  difficulty: basic
  timelimit: 600
- slug: rolling-update
  id: 2vakukxvlho2
  type: challenge
  title: Do a Rolling Update of the Chat Job
  teaser: |
    Do a rolling update of the Chat job.
  assignment: |-
    In this challenge, you will do a rolling update of the Chat job.

    Please navigate back to the /root/nomad/jobs directory on the "Server" tab again with this command:
    ```
    cd /root/nomad/jobs
    ```

    Next, you need to modify the "chat-app.nomad" job specification to use a different tag on the "lhaig/anon-app" image. Edit the file on the "Jobs" tab and change the "0.02" tag to "dark-0.02". Then click the disk icon above the file to save it.

    If you prefer, you could just run this command instead of editing the file yourself:
    ```
    sed -i "s/anon-app:0.02/anon-app:dark-0.02/g" chatapp.nomad
    ```

    Now, check what would happen if you redeployed the application with this command:
    ```
    nomad job plan chatapp.nomad
    ```

    This should return the following text:<br>
    `
    +/- Job: "chat-app"
    +/- Task Group: "chat-app" (1 create/destroy update, 2 ignore)
      +/- Task: "chat-app" (forces create/destroy update)
        +/- Config {
          +/- image: "lhaig/anon-app:0.02" => "lhaig/anon-app:dark-0.02"
            }
          Task: "connect-proxy-chat-app"
    `<br>
    This indicates that Nomad would initially only update 1 of the 3 allocations, confirming that it would do a rolling update.

    Go ahead and actually run a rolling update of the chat-app application:
    ```
    nomad job run chatapp.nomad
    ```

    In the Nomad UI, if you click the Instruqt refresh button and then select the "chat-app" job, you will see that there is a new deployment running. Over the next few minutes, you will see the numbers of placed and healthy allocations for this deployment increase to 3.

    Alternatively, you could monitor the progress of the rolling update by periodically running `nomad deployment status <deployment_id>` where <deployment_id\> is the ID of the latest deployment shown in the Nomad UI.

    As the number of healthy allocations increases, refresh the "Chat 1", "Chat 2", and "Chat 3" tabs to see what happens. The apps should change from a light to a dark background one at a time as the rolling update proceeds. The color of the "Send" button in the app will change from green to purple. Note that the order in which the apps are updated can vary.

    If you happen to catch one of the apps while it is being redeployed, you will see a "Connecting to the service..." message. That is ok.

    Since the chat messages are stored in the MongoDB database, the messages you previously sent will still show up.

    After all the chat-app allocations are healthy and the chat-app clients have the dark background, run the following command on the "Server" tab:
    ```
    nomad job status chat-app
    ```
    You will now see 6 allocations, 3 of which are marked "complete" while 3 are "running".

    In the next challenge, you will update the Chat app using a blue/green deployment strategy.
  notes:
  - type: text
    contents: |-
      In this challenge, you will do a rolling update of the Chat job.

      In the next challenge, you will update the Chat app using a blue/green deployment strategy.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Fabio UI
    type: service
    hostname: nomad-client-1
    port: 9998
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 9002
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 9002
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 9002
  difficulty: basic
  timelimit: 3600
- slug: blue-green-deployment
  id: 2o5tzi8hyd5f
  type: challenge
  title: Do a Blue/Green Deployment of the Chat Job
  teaser: |
    Do a blue/green deployment of the Chat job.
  assignment: |-
    In this challenge, you will do a blue/green deployment of the Chat job.

    Please navigate back to the /root/nomad/jobs directory on the "Server" tab again with this command:
    ```
    cd /root/nomad/jobs
    ```

    Edit the "chat-app.nomad" job specification file on the "Jobs" tab, making the following changes:

      1. Change the "dark-0.02" tag on the image back to "0.02".
      2.

    In the next challenge, you will update the Chat job using a canary deployment.
  notes:
  - type: text
    contents: |-
      In this challenge, you will do a blue/green deployment of the Chat job.

      In the next challenge, you will update the Chat job using a canary deployment.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Fabio UI
    type: service
    hostname: nomad-client-1
    port: 9998
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 9002
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 9002
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 9002
  difficulty: basic
  timelimit: 3600
- slug: canary-deployment
  id: 8sbd1wqgtqir
  type: challenge
  title: Do a Canary Deployment of the Chat Job
  teaser: |
    Do a canary deployment of the Chat job.
  assignment: |-
    In this challenge, you will do a canary deployment of the Chat job.

    Congratulations on completing the Nomad Job Update Strategies track!
  notes:
  - type: text
    contents: |-
      In this challenge, you will do a canary deployment of the Chat job.

      In a canary deployment, one or more additional allocations of the job are deployed while Nomad continues to run the original allocations. This gives operators a chance to verify that the new version of the job is behaving correctly before completely replacing the old version.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Fabio UI
    type: service
    hostname: nomad-client-1
    port: 9998
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 9002
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 9002
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 9002
  difficulty: basic
  timelimit: 3600
checksum: "13723511898125120825"
