slug: job-placement-with-constraints-affinities-and-spread
id: ypw13j54qi0v
type: track
title: Nomad Advanced Job Placement
teaser: Explore Nomad Job Placement strategies with Constraints, Affinities, and Spread
description: |-
  This track will show how to use Nomad's Job Placement with Constraints, Affinities, and Spread to demonstrate the complexity with wihch you can control your job placement.

  In this track you will deploy a nomad cluster with a WebApp and [Trafek](https://containo.us/traefik/) for networking.

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics) and [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-frontend/img/tracks/default.png
tags:
- nomad
- advanced
owner: hashicorp
developers:
- cdunlap@hashicorp.com
private: true
published: false
maintenance: true
challenges:
- slug: verify-nomad-cluster-health
  id: s04zptitbxqu
  type: challenge
  title: Verify the Health of Your Nomad Enterprise Cluster
  teaser: |
    Verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts.
  assignment: |-
    In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 3 Nomad/Consul clients. The [Consul Connect integration](https://nomadproject.io/guides/integrations/consul-connect) has been enabled on the server and clients since the chatapp uses it to talk to the MongoDB database. Additionally, a Nomad [host volume](https://nomadproject.io/docs/configuration/client/#host_volume-stanza) called "mongodb_mount" has been configured on the clients under the /opt/mongodb/data directory so that MongoDB can persist its data.

    First, verify that all 4 Consul agents are running and connected to the cluster by running this command on the "Server" tab:
    ```
    consul members
    ```
    You should see 4 Consul agents with the "alive" status.

    Check that the Nomad server is running by running this command on the "Server" tab:
    ```
    nomad server members
    ```
    You should see 1 Nomad server with the "alive" status.

    Check the status of the Nomad client nodes by running this command on the "Server" tab:
    ```
    nomad node status
    ```
    You should see 3 Nomad clients with the "ready" status.

    You can also check the status of the Nomad server and clients in the Nomad and Consul UIs.

    In the next challenge, you will configure Nomad Enterprise namespaces and resource quotas.
  notes:
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

      In later challenges, you will run a nomad job for a "webapp" and a "traefik" jobs and update them using Nomad Job Placement stanzas Constraint, Affinity, and  Spread.
  tabs:
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Client
    type: terminal
    hostname: nomad-client-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 300
- slug: deploy-the-jobs
  id: ak0ehohkwlsk
  type: challenge
  title: Deploy the WebApp and Traefik
  teaser: |
    Deploy a WebApp and Traefik
  assignment: |-
    In this challenge, we will introduce Traefik ![alt text](https://containo.us/assets/img/traefik-logo.svg)


    You will deploy a WebApp with 3 instances and Traefik to enable communications.

    ##
    Run the webapp.nomad Job
    Inspect the "webapp.nomad" job specification file
    on the "Jobs" tab. This will deploy a WebApp.

    Navigate to the /root/nomad/jobs
    directory on the "Server" tab with this command:
    ```
    cd /root/nomad/jobs
    ```

    Run
    the "webapp.nomad" job with this command on the "Server" tab:
    ```
    nomad job run webapp.nomad
    ```
    This should return something like this:<br>
    `
    ==>
    Monitoring evaluation "e86a91bb"
    Evaluation triggered by job "demo-webapp"
    Evaluation
    within deployment: "92da48d4"
    Allocation "1feff487" created: node "530789ff",
    group "demo"
    Allocation "e8082da6" created: node "21d0cdba", group "demo"
    Allocation
    "1a423ab1" created: node "edca4722", group "demo"
    Evaluation status changed:
    "pending" -> "complete"
    ==> Evaluation "e86a91bb" finished with status
    "complete"
    `<br>

    You can check the job in the Nomad UI after waiting 15-30
    seconds, clicking the Instruqt refresh button above the Nomad UI, and then selecting
    the "webapp" job. You might need to make your browser window a bit wider to
    see the UI properly and so that the Instruqt buttons do not overlap the Nomad
    UI tab. You could also click the rectangular button next to the refresh button
    to hide this assignment. After no more than 1 minute, you should see that the
    job has 1 healthy allocation.

    Alternatively, you can run the command:
    ```
    nomad job status webapp
    ```
    on the "Server" tab to check the status of the "webapp" jobs and validate that the "webapp" task group is healthy.

    ## Run the traefik.nomad
    Job Inspect the "traefik.nomad" job specification file on the "Jobs" tab. This will deploy Traefik.

    Run the "traefik.nomad" job with this command on the "Server" tab:
    ```
    nomad job run traefik.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "21f6eb47"
    Evaluation
    triggered by job "traefik"
    Evaluation within deployment: "14c4b0cc"
    Allocation
    "9a123921" created: node "edca4722", group "traefik"
    Evaluation status
    changed: "pending" -> "complete"
    ==> Evaluation "21f6eb47" finished with
    status "complete"
    `<br>

    You can check the job in the Nomad UI after waiting
    15-30 seconds, clicking the Instruqt refresh button above the Nomad UI, and then
    selecting the "traefik" job. You might need to make your browser window a bit
    wider to see the UI properly and so that the Instruqt buttons do not overlap the
    Nomad UI tab. You could also click the rectangular button next to the refresh
    button to hide this assignment. After no more than 1 minute, you should see that
    the job has 1 healthy allocation.

    Alternatively, you can run the command:
    ```
    nomad
    job status traefik
    ```
    on the "Server" tab to check the status of the "traefik" job and validate that the "REPLACEME" task group is healthy.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy a webapp and Traefik

      In later challenges, you will run `UPDATETHIS`
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Client
    type: terminal
    hostname: nomad-client-1
  - title: Client
    type: terminal
    hostname: nomad-client-2
  - title: Client
    type: terminal
    hostname: nomad-client-3
  - title: Client
    type: terminal
    hostname: nomad-client-4
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Traefik UI
    type: service
    hostname: nomad-client-1
    port: 8081
  difficulty: basic
  timelimit: 3600
- slug: show-constraint
  id: lnfmntfw6so3
  type: challenge
  title: Change the Spread stanza to Constraint
  teaser: |
    Change the Spread stanza to Constraint and get nodes to run on same instances
  assignment: |-
    In this challenge, you will change out the Spread stanza with the Constraint stanza demonstrating how we can bind jobs to hosts in other ways.


    Please navigate back to the /root/nomad/jobs directory on the "Server" tab again with this command:
    ```
    cd /root/nomad/jobs
    ```

    Edit the "chat-app.nomad" job specification file on the "Jobs" tab, making the following changes:

    --edited this out for now  1. Change the count to 4
          `count = 4`
      1. Change the out the Spread stanza <br>
          `
          spread {
          attribute = "${node.unique.name}"
          }
        ` <br>
        With this new Constraint stanza
        ```
        constraint
          {
          attribute = "${attr.platform.gce.machine-type}"
          value     = "n1-standard-1"
          }
        ```
    Now, check what would happen if you redeployed the "chat-app" job with this command on the "Server" tab:
    ```
    nomad job
    plan chat-app.nomad
    ```

    This should return something like this:<br>
    `
     +/-
    Job: "chat-app"
     +/- Task Group: "chat-app" (1 create/destroy update, 2 ignore)
     + Constraint {
         LTarget: ""
       + Operand: "distinct_hosts"
       + RTarget:
    "true" }
       Task: "chat-app"
       Task: "connect-proxy-chat-app"
    `<br>

    The plan indicates that Nomad would update 1 based on increasing the count and Go ahead and re-run the "chat-app" job with this command:
    ```
    nomad job run chat-app.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "0fd02bb5"
        Evaluation triggered by job "chat-app"
        Evaluation within deployment: "fa80df0c"
        Allocation "7633b926" created: node "14e04978", group "chat-app"
        Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "0fd02bb5" finished with status "complete"
    `<br>
    If you look at the chat-app job in the Nomad UI, you will see that there are 4 allocations running.
  notes:
  - type: text
    contents: In this challenge, you will do Update the number of chat apps and change
      out thea Spread stanza with the Constraint stanza. You will also pin the DB
      server to one host based on Constraint.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 8080
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 8080
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 8080
  difficulty: basic
  timelimit: 3600
checksum: "2770340027653176769"
