slug: job-placement-with-constraints-affinities-and-spread
id: ypw13j54qi0v
type: track
title: Nomad Advanced Job Placement
teaser: Explore Nomad Job Placement strategies with Constraints, Affinities, and Spread
description: |-
  This track will show how to use Nomad's Job Placement with Constraints, Affinities, and Spread to demonstrate the complexity with which you can control your job placement.

  In this track you will deploy a nomad cluster with a WebApp and [Traefik](https://containo.us/traefik/) for networking.

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics) and [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-frontend/img/tracks/default.png
tags:
- nomad
- advanced
owner: hashicorp
developers:
- cdunlap@hashicorp.com
private: true
published: false
maintenance: true
challenges:
- slug: verify-nomad-cluster-health
  id: s04zptitbxqu
  type: challenge
  title: Verify the Health of Your Nomad Enterprise Cluster
  teaser: |
    Verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts.
  assignment: |-
    In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 3 Nomad/Consul clients. The [Consul Connect integration](https://nomadproject.io/guides/integrations/consul-connect) has been enabled on the server and clients since the chatapp uses it to talk to the MongoDB database. Additionally, a Nomad [host volume](https://nomadproject.io/docs/configuration/client/#host_volume-stanza) called "mongodb_mount" has been configured on the clients under the /opt/mongodb/data directory so that MongoDB can persist its data.

    First, verify that all 4 Consul agents are running and connected to the cluster by running this command on the "Server" tab:
    ```
    consul members
    ```
    You should see 4 Consul agents with the "alive" status.

    Check that the Nomad server is running by running this command on the "Server" tab:
    ```
    nomad server members
    ```
    You should see 1 Nomad server with the "alive" status.

    Check the status of the Nomad client nodes by running this command on the "Server" tab:
    ```
    nomad node status
    ```
    You should see 3 Nomad clients with the "ready" status.

    You can also check the status of the Nomad server and clients in the Nomad and Consul UIs.

    In the next challenge, you will configure Nomad Enterprise namespaces and resource quotas.
  notes:
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

      In later challenges, you will run a nomad job for a "webapp" and a "traefik" jobs and update them using Nomad Job Placement stanzas Constraint, Affinity, and Spread.
  tabs:
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 300
- slug: deploy-the-jobs
  id: ak0ehohkwlsk
  type: challenge
  title: Deploy the WebApp and Traefik
  teaser: |
    Deploy a WebApp and Traefik
  assignment: |-
    ![alt text](https://containo.us/assets/img/traefik-logo.svg)<br>
    In this challenge, we will introduce Traefik to be used as a proxy for our webapps.

    You will deploy a webapp with 3 instances and Traefik to enable communications.

    ## Inspect the Nomad Jobs.
    Lets begin by inspecting the Nomad jobs and getting familiar with what's going to be deployed.

    ### Inspect webapp.nomad
    Inspect the "webapp.nomad" job specification file on the "Jobs" tab. This will deploy 3 webapps to our Nomad cluster.
    <br>

    One instance of the webapp task group will be deployed to each Nomad client because of the job's use of the "spread" stanza:<br>
    `
    spread {
      attribute = "\${node.unique.name}"
    }
    `<br>

    Nomad selects a dynamic port for each webapp, in order to obtain those dynamic values, this job uses [variable interpolation](https://nomadproject.io/docs/runtime/interpolation/) for getting the port and the IP of each running webapp. This is achieved in combination with the [port parameters](https://nomadproject.io/docs/job-specification/network/#port-parameters) in the network stanza.

    ## Run the webapp.nomad Job
    Navigate to the /root/nomad/jobs directory on the "Server" tab with this command:
    ```
    cd /root/nomad/jobs
    ```

    Run the "webapp.nomad" job with this command on the "Server" tab:
    ```
    nomad job run webapp.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "a05672bc"
    Evaluation triggered by job "webapp"
    Evaluation within deployment: "5692a28d"
    Allocation "6bc9d9e6" created: node "33fd8505", group "webapp"
    Allocation "1b90c684" created: node "3006bb6d", group "webapp"
    Allocation "56b0671c" created: node "2f4a35ac", group "webapp"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "a05672bc" finished with status "complete"
    `<br>

    ### Monitoring progress of the webapp job deployment
    For a UI view of the progress you can check the job in the "Nomad UI" tab and after waiting 15-30
    seconds if you don't see any changes in the Nomad jobs page, you can try clicking the Instruqt refresh button above the Nomad UI, and then select
    the "webapp" job.
    <br>
    You might need to make your browser window a bit wider to see the UI properly and so that the Instruqt buttons do not overlap the "Nomad UI" tab.
    <br>
    You could also click the rectangular button next to the refresh button to hide this assignment. After no more than 1 minute, you should see that the job has 3 healthy allocation.

    Alternatively, to view the status in the CLI, you can run the command:
    ```
    nomad job status webapp
    ```
    on the "Server" tab to check the status of the "webapp" jobs and validate that the "webapp" task group shows 3 healthy healthy.
    <br>
    You can also inspect the "Consul UI" tab to see the health of the webapps that are running as services in Consul.

    ### Inspect traefik.nomad
    Inspect the "traefik.nomad" job specification file on the "Jobs" tab. This will deploy a Traefik container that will proxy all requests to the webapps on 8080 to their dynamic ports allocated by Nomad.
    <br>
    This job is utilizing the [constraints config](https://docs.traefik.io/providers/consul-catalog/#constraints) for Consul Catalog in Traefik.
    <br>
    `constraints = ["tag==service"]`


    If you take a look back at the webapp.nomad on line 30 you will see the tags being applied.
    <br>
    `
    tags = [
          "traefik.tags=service",
          "traefik.frontend.rule=PathPrefixStrip:/myapp",
        ]
    `<br>

    ## Run the traefik.nomad Job
    Run the "traefik.nomad" job with this command on the "Server" tab:
    ```
    nomad job run traefik.nomad
    ```
    This should return something like this:
    <br>
    `
    ==> Monitoring evaluation "6765c131"
    Evaluation triggered by job "traefik"
    Evaluation within deployment: "d15e6190"
    Allocation "0e36e38a" created: node "44d88b4b", group "traefik"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "6765c131" finished with status "complete"
    `<br>

    For a UI view of the progress you can check the job in the "Nomad UI" tab and after waiting 15-30
    seconds if you don't see any changes in the Nomad jobs page, you can try clicking the Instruqt refresh button above the Nomad UI, and then select
    the "traefik" job.
    <br>
    You might need to make your browser window a bit wider to see the UI properly and so that the Instruqt buttons do not overlap the "Nomad UI" tab.
    <br>
    You could also click the rectangular button next to the refresh button to hide this assignment. After no more than 1 minute, you should see that the job has 1 healthy allocation.

    Alternatively, you can run the command:
    ```
    nomad job status traefik
    ```
    on the "Server" tab to check the status of the "traefik" job and validate that the "traefik" task group is healthy.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy a webapp and Traefik

      In later challenges, you will learn about Constraints and Affinities .
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Client1
    type: terminal
    hostname: nomad-client-1
  - title: Client2
    type: terminal
    hostname: nomad-client-2
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Traefik UI
    type: service
    hostname: nomad-client-1
    port: 8081
  difficulty: basic
  timelimit: 3600
- slug: show-constraint
  id: lnfmntfw6so3
  type: challenge
  title: Change the Spread stanza to Constraint
  teaser: |
    Change the Spread stanza to Constraint and get nodes to run on same instances
  assignment: |-
    In this challenge, you will change out the Spread stanza with the Constraint stanza demonstrating how we can bind jobs to hosts in other ways.


    Please navigate back to the /root/nomad/jobs directory on the "Server" tab again with this command:
    ```
    cd /root/nomad/jobs
    ```

    Edit the "chat-app.nomad" job specification file on the "Jobs" tab, making the following changes:

    --edited this out for now  1. Change the count to 4
          `count = 4`
      1. Change the out the Spread stanza <br>
          `
          spread {
          attribute = "${node.unique.name}"
          }
        ` <br>
        With this new Constraint stanza
        ```
        constraint
          {
          attribute = "${attr.platform.gce.machine-type}"
          value     = "n1-standard-1"
          }
        ```
    Now, check what would happen if you redeployed the "chat-app" job with this command on the "Server" tab:
    ```
    nomad job
    plan chat-app.nomad
    ```

    This should return something like this:<br>
    `
     +/-
    Job: "chat-app"
     +/- Task Group: "chat-app" (1 create/destroy update, 2 ignore)
     + Constraint {
         LTarget: ""
       + Operand: "distinct_hosts"
       + RTarget:
    "true" }
       Task: "chat-app"
       Task: "connect-proxy-chat-app"
    `<br>

    The plan indicates that Nomad would update 1 based on increasing the count and Go ahead and re-run the "chat-app" job with this command:
    ```
    nomad job run chat-app.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "0fd02bb5"
        Evaluation triggered by job "chat-app"
        Evaluation within deployment: "fa80df0c"
        Allocation "7633b926" created: node "14e04978", group "chat-app"
        Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "0fd02bb5" finished with status "complete"
    `<br>
    If you look at the chat-app job in the Nomad UI, you will see that there are 4 allocations running.
  notes:
  - type: text
    contents: In this challenge, you will do Update the number of chat apps and change
      out thea Spread stanza with the Constraint stanza. You will also pin the DB
      server to one host based on Constraint.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 8080
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 8080
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 8080
  difficulty: basic
  timelimit: 3600
checksum: "2574787986773270289"
