slug: job-placement
id: ypw13j54qi0v
type: track
title: Nomad Advanced Job Placement
teaser: Explore Nomad Job Placement strategies with Constraints, Affinities, and Spread
description: |-
  This track will show how to use Nomad's Job Placement with Constraints, Affinities, and Spread to demonstrate the complexity with which you can control your job placement.

  In this track you will deploy a nomad cluster with a WebApp and [Traefik](https://containo.us/traefik/) for networking.

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics) and [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-frontend/img/tracks/default.png
tags:
- nomad
- advanced
owner: hashicorp
developers:
- cdunlap@hashicorp.com
private: true
published: false
maintenance: true
challenges:
- slug: verify-nomad-cluster-health
  id: s04zptitbxqu
  type: challenge
  title: Verify the Health of Your Nomad Enterprise Cluster
  teaser: |
    Verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts.
  assignment: |-
    In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 3 Nomad/Consul clients.

    First, verify that all 4 Consul agents are running and connected to the cluster by running this command on the "Server" tab:
    ```
    consul members
    ```
    You should see 4 Consul agents with the "alive" status.

    Check that the Nomad server is running by running this command on the "Server" tab:
    ```
    nomad server members
    ```
    You should see 1 Nomad server with the "alive" status.

    Check the status of the Nomad client nodes by running this command on the "Server" tab:
    ```
    nomad node status
    ```
    You should see 3 Nomad clients with the "ready" status.

    You can also check the status of the Nomad server and clients in the Nomad and Consul UIs.

    In the next challenge, you will configure Nomad Enterprise namespaces and resource quotas.
  notes:
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

      In later challenges, you will run a nomad job for a "webapp" and a "traefik" jobs and update them using Nomad Job Placement stanzas Constraint, Affinity, and Spread.
  tabs:
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 300
- slug: deploy-the-jobs
  id: ak0ehohkwlsk
  type: challenge
  title: Deploy the WebApp and Traefik
  teaser: |
    Deploy a WebApp and Traefik
  assignment: |-
    ![alt text](https://containo.us/assets/img/traefik-logo.svg)<br>
    In this challenge, we will introduce Traefik to be used as a proxy for our webapps.

    You will deploy a webapp with 3 instances and Traefik to enable communications.

    ## Inspect the Nomad Jobs.
    Lets begin by inspecting the Nomad jobs and getting familiar with what's going to be deployed.

    ### Inspect webapp.nomad
    Inspect the "webapp.nomad" job specification file on the "Jobs" tab. This will deploy 3 webapps to our Nomad cluster.
    <br>

    One instance of the webapp task group will be deployed to each Nomad client because of the job's use of the "spread" stanza:<br>
    `
    spread {
      attribute = "\${node.unique.name}"
    }
    `<br>

    Nomad selects a dynamic port for each webapp, in order to obtain those dynamic values, this job uses [variable interpolation](https://nomadproject.io/docs/runtime/interpolation/) for getting the port and the IP of each running webapp. This is achieved in combination with the [port parameters](https://nomadproject.io/docs/job-specification/network/#port-parameters) in the network stanza.

    ## Run the webapp.nomad Job
    Navigate to the /root/nomad/jobs directory on the "Server" tab with this command:
    ```
    cd /root/nomad/jobs
    ```

    Run the "webapp.nomad" job with this command on the "Server" tab:
    ```
    nomad job run webapp.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "a05672bc"
    Evaluation triggered by job "webapp"
    Evaluation within deployment: "5692a28d"
    Allocation "6bc9d9e6" created: node "33fd8505", group "webapp"
    Allocation "1b90c684" created: node "3006bb6d", group "webapp"
    Allocation "56b0671c" created: node "2f4a35ac", group "webapp"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "a05672bc" finished with status "complete"
    `<br>

    ### Monitoring progress of the webapp job deployment
    For a UI view of the progress you can check the job in the "Nomad UI" tab and after waiting 15-30
    seconds if you don't see any changes in the Nomad jobs page, you can try clicking the Instruqt refresh button above the Nomad UI, and then select
    the "webapp" job.
    <br>
    You might need to make your browser window a bit wider to see the UI properly and so that the Instruqt buttons do not overlap the "Nomad UI" tab.
    <br>
    You could also click the rectangular button next to the refresh button to hide this assignment. After no more than 1 minute, you should see that the job has 3 healthy allocation.

    Alternatively, to view the status in the CLI, you can run the command:
    ```
    nomad job status webapp
    ```
    on the "Server" tab to check the status of the "webapp" jobs and validate that the "webapp" task group shows 3 healthy healthy.
    <br>
    You can also inspect the "Consul UI" tab to see the health of the webapps that are running as services in Consul.

    ### Inspect traefik.nomad
    Inspect the "traefik.nomad" job specification file on the "Jobs" tab. This will deploy a Traefik container that will proxy all requests to the webapps on 8080 to their dynamic ports allocated by Nomad.
    <br>
    This job is utilizing the [constraints config](https://docs.traefik.io/providers/consul-catalog/#constraints) for Consul Catalog in Traefik.
    <br>
    `constraints = ["tag==service"]`


    If you take a look back at the webapp.nomad on line 30 you will see the tags being applied.
    <br>
    `
    tags = [
          "traefik.tags=service",
          "traefik.frontend.rule=PathPrefixStrip:/myapp",
        ]
    `<br>

    ## Run the traefik.nomad Job
    Run the "traefik.nomad" job with this command on the "Server" tab:
    ```
    nomad job run traefik.nomad
    ```
    This should return something like this:
    <br>
    `
    ==> Monitoring evaluation "6765c131"
    Evaluation triggered by job "traefik"
    Evaluation within deployment: "d15e6190"
    Allocation "0e36e38a" created: node "44d88b4b", group "traefik"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "6765c131" finished with status "complete"
    `<br>

    For a UI view of the progress you can check the job in the "Nomad UI" tab and after waiting 15-30
    seconds if you don't see any changes in the Nomad jobs page, you can try clicking the Instruqt refresh button above the Nomad UI, and then select
    the "traefik" job.
    <br>
    You might need to make your browser window a bit wider to see the UI properly and so that the Instruqt buttons do not overlap the "Nomad UI" tab.
    <br>
    You could also click the rectangular button next to the refresh button to hide this assignment. After no more than 1 minute, you should see that the job has 1 healthy allocation.

    Alternatively, you can run the command:
    ```
    nomad job status traefik
    ```
    on the "Server" tab to check the status of the "traefik" job and validate that the "traefik" task group is healthy.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy a webapp and Traefik

      In later challenges, you will learn about Constraints and Affinities .
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Client1
    type: terminal
    hostname: nomad-client-1
  - title: Client2
    type: terminal
    hostname: nomad-client-2
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  difficulty: basic
  timelimit: 1600
- slug: use-spread
  id: sovmjupmykql
  type: challenge
  title: Use the Spread stanza
  teaser: |
    Use the Spread stanza to distribute load across your Nomad fleet
  assignment: |-
    In this challenge, you will use the Spread stanza to demonstrate how we can increase the failure tolerance of their applications by specifying a node attribute that allocations should be spread over.
    This allows operators to spread allocations over attributes such as datacenter, availability zone, or even rack in a physical datacenter. By default, when using spread the scheduler will attempt to place allocations equally among the available values of the given target.

    Please navigate back to the /root/nomad/jobs directory on the "Server" tab again with this command:
    ```
    cd /root/nomad/jobs
    ```

    Edit the "webapp.nomad" job specification file on the "Jobs" tab, making the following changes:
    <br><br>
    Find the config line that says
    <br>
    `count = 6`
    <br>
    Add the Spread stanza after the count line: <br>
    ```
      spread {
      attribute = "${node.unique.name}"
      }
    ```
    <br>
    Now, check what would happen if you redeployed the "webapp" job with this command on the "Server" tab:<br>
    ```
    nomad job plan webapp.nomad
    ```<br>

    The output of this command will vary based on how Nomad decided to spread the nodes across the hosts. Normally this output will declare that it will create/destroy some Task/Task Group in order to now ballance out the workload.
    <br>
    You can view the current client workload allocations that have registered with Consul in order to see how the workloads are distributed. Navigate to the "Consul UI" Tab and click on the webapp service and take note of the "Node" column in the output.
    <br>
    <br>

    Now let's run the job and see the changes in effect.
    ```
    nomad job run webapp.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "cd1d28a8"
    Evaluation triggered by job "webapp"
    Evaluation within deployment: "f374d3b9"
    Allocation "cf30600e" created: node "e248cdaf", group "webapp"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "cd1d28a8" finished with status "complete"
    `<br>
    You can again for to the Consul UI and see the webapp allocations changing in real time. When all jobs have registered and are marked as healthy from Consul you should now see 2 webapp jobs distributed evenly on each client.
    <br>
    If you look at the webapp job in the Nomad UI, you will see that there are 6 allocations running.
  notes:
  - type: text
    contents: In this challenge, you will do Update the logic that is choosing how
      to distribute the workloads to use the Spread stanza to evenly distribute across
      the Nomad fleet.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: webapp 1
    type: service
    hostname: nomad-client-1
    port: 8080
  difficulty: basic
  timelimit: 3600
- slug: use-constraint
  id: uowivhtikmoz
  type: challenge
  title: Add the Constraint stanza to Traefik to bind it to a host.
  teaser: |
    Use the Constraint stanza to pin Traefik workload to a specific type host.
  assignment: |-
    In this challenge, use the Constraint stanza to allows restricting the set of eligible nodes. We will be using a Constraint that filters on [attributes](https://www.nomadproject.io/docs/runtime/interpolation), but you could also use [client metadata](https://www.nomadproject.io/docs/configuration/client#custom-metadata-network-speed-and-node-class) as filters as well.


    Please navigate back to the /root/nomad/jobs directory on the "Server" tab again with this command:
    ```
    cd /root/nomad/jobs
    ```

    Edit the "traefik.nomad" job specification file on the "Jobs" tab, making the following changes:
    <br>
    Find the line in with teh task definition<br>
    `task "traefik" {`<br>
    and add the constraint after that.<br>
    Add this new Constraint stanza
    ```
    constraint
      {
      attribute = "${attr.platform.gce.machine-type}"
      value     = "n1-standard-2"
      }
    ```
    Now, check what would happen if you redeployed the "traefik" job with this command on the "Server" tab:
    ```
    nomad job plan traefik.nomad
    ```

    This should return something like this:<br>
    `
    REPLACE ME WITH OUTPUT
    `<br>

    The plan indicates that Nomad would update 1 based on forcing the alocation over to a n1-standard-2 node.
    ```
    nomad job run traefik.nomad
    ```
    This should return something like this:<br>
    `
    REPLACE ME WITH OUTPUT
        Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "0fd02bb5" finished with status "complete"
    `<br>
    If you look at the traefik job in the Nomad UI, you will see that there is 1 allocation running.
  notes:
  - type: text
    contents: In this challenge, you will do Update the Traefik job to run on a specific
      host type in the cloud provider.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: Traefik UI
    type: service
    hostname: nomad-client-1
    port: 8081
  difficulty: basic
  timelimit: 3600
checksum: "5121767812337620124"
