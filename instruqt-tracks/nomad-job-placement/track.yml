slug: job-placement-with-constraints-affinities-and-spread
id: ypw13j54qi0v
type: track
title: Nomad Advanced Job Placement
teaser: Explore Nomad Job Placement strategies with Constraints, Affinities, and Spread
description: |-
  This track will show how to use Nomad's Job Placement with Constraints, Affinities, and Spread to demonstrate the complexity with wihch you can control your job placement.

  In this track you will deploy a nomad cluster with a Chat App, MongoDB, and Trafik

  Before running this track, we suggest you run the [Nomad Basics](https://instruqt.com/hashicorp/tracks/nomad-basics) and [Nomad Simple Cluster](https://instruqt.com/hashicorp/tracks/nomad-simple-cluster) tracks.
icon: https://storage.googleapis.com/instruqt-frontend/img/tracks/default.png
tags:
- nomad
- advanced
owner: hashicorp
developers:
- cdunlap@hashicorp.com
private: true
published: false
maintenance: true
challenges:
- slug: verify-nomad-cluster-health
  id: s04zptitbxqu
  type: challenge
  title: Verify the Health of Your Nomad Enterprise Cluster
  teaser: |
    Verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts.
  assignment: |-
    In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

    The cluster is running 1 Nomad/Consul server and 3 Nomad/Consul clients. The [Consul Connect integration](https://nomadproject.io/guides/integrations/consul-connect) has been enabled on the server and clients since the chatapp uses it to talk to the MongoDB database. Additionally, a Nomad [host volume](https://nomadproject.io/docs/configuration/client/#host_volume-stanza) called "mongodb_mount" has been configured on the clients under the /opt/mongodb/data directory so that MongoDB can persist its data.

    First, verify that all 4 Consul agents are running and connected to the cluster by running this command on the "Server" tab:
    ```
    consul members
    ```
    You should see 4 Consul agents with the "alive" status.

    Check that the Nomad server is running by running this command on the "Server" tab:
    ```
    nomad server members
    ```
    You should see 1 Nomad server with the "alive" status.

    Check the status of the Nomad client nodes by running this command on the "Server" tab:
    ```
    nomad node status
    ```
    You should see 3 Nomad clients with the "ready" status.

    You can also check the status of the Nomad server and clients in the Nomad and Consul UIs.

    In the next challenge, you will configure Nomad Enterprise namespaces and resource quotas.
  notes:
  - type: text
    contents: |-
      In this challenge, you will verify the health of the Nomad Enterprise cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.

      In later challenges, you will run "mongodb", "chat-app", and "nginx" jobs and update the second using rolling, blue/green, and canary update strategies. nginx will be used as a load balancer that will forward requests to instances of the "chat" application which will store its messages in a single MongoDB database.
  tabs:
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: nginx UI
    type: service
    hostname: nomad-server-1
    port: 8081
  difficulty: basic
  timelimit: 300
- slug: deploy-the-jobs
  id: s21kxxwztur9
  type: challenge
  title: Deploy the MongoDB, nginx, and Chat-App Jobs
  teaser: |
    Deploy the MongoDB, nginx, and Chat-App jobs.
  assignment: |-
    In this challenge, you will deploy the "mongodb", "chat-app", and "nginx" jobs.

    ## Run the mongodb.nomad Job
    Inspect the "mongodb.nomad" job specification file on the "Jobs" tab. This will deploy a "db" task group that runs the MongoDB database on port 27017 from the "mongo" Docker image.

    Note that the database persists its data to the Nomad volume "mongodb_vol" which uses the "mongodb_mount" host volume that was configured on each Nomad client. The "mongodb_vol" volume is mounted inside the Docker container on the path "/data/db".

    Navigate to the /root/nomad/jobs directory on the "Server" tab with this command:
    ```
    cd /root/nomad/jobs
    ```

    Run the "mongodb.nomad" job with this command on the "Server" tab:
    ```
    nomad job run mongodb.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "64754ee4"
    Evaluation triggered by job "mongodb"
    Evaluation within deployment: "baa2ea28"
    Allocation "60fde323" created: node "6777b2ca", group "db"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "64754ee4" finished with status "complete"
    `<br>

    You can check the job in the Nomad UI after waiting 15-30 seconds, clicking the Instruqt refresh button above the Nomad UI, and then selecting the "mongodb" job. You might need to make your browser window a bit wider to see the UI properly and so that the Instruqt buttons do not overlap the Nomad UI tab. You could also click the rectangular button next to the refresh button to hide this assignment. After no more than 1 minute, you should see that the job has 1 healthy allocation.

    Alternatively, you can run the command:
    ```
    nomad job status mongodb
    ```
    on the "Server" tab to check the status of the "mongodb" job and validate that the "db" task group is healthy.

    ## Run the chat-app.nomad Job
    Next, inspect the "chat-app.nomad" job specification file on the "Jobs" tab. This job deploys 3 instances of a "chat-app" task group that runs a custom Docker image, "lhaig/anon-app:light-0.03" created by two HashiCorp solutions engineers, Guy Barrows and Lance Haig. Note that the job specifies the "light-0.03" tag for that image to use a version of the chat app with a light background. Later, when we update the app in various ways, we will also use the tag, "dark-0.03", to use a version of the app with a dark background.

    One instance of the chat-app task group will be deployed to each Nomad client because of the job's use of the "spread" stanza:<br>
    `
    spread {
      attribute = "${node.unique.name}"
    }
    `<br>
    Later, when we do a blue-green deployment, we will see that the new "green" deployments will also be spread evenly across the 3 Nomad clients.

    The "chat-app" task runs on a dynamic port selected by Nomad; that port is mapped to port 5000 inside the task group's "network" stanza:<br>
    `
    network {
      mode = "bridge"
      port "http" {
        to = 5000
      }
    }
    `<br>
    The nginx load balancer that you will deploy after the "chat-app" job will automatically add a route for each instance of the chat app because of the "template" stanza within the "nginx.nomad" job specification file.

    Finally, the "chat-app" task group uses Consul Connect [sidecar proxies](https://www.consul.io/docs/connect/proxies.html) to talk to the MongoDB database using mutual Transport Layer Security (mTLS) certificates. This is implemented by this stanza:<br>
    `
    connect {
      sidecar_service {
        tags = ["chat-app-proxy"]
        proxy {
          upstreams {
            destination_name = "mongodb"
            local_bind_port = 27017
          }
        }
      }
    }
    `<br>

    Run the "chat-app.nomad" job on the "Server" tab with this command:
    ```
    nomad job run chat-app.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "f08ca561"
    Evaluation triggered by job "chat-app"
    Evaluation within deployment: "5dc975d3"
    Allocation "5d57baab" created: node "9afadcdd", group "chat-app"
    Allocation "14a5cc97" created: node "6777b2ca", group "chat-app"
    Allocation "2f792469" created: node "4b19ed9a", group "chat-app"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "f08ca561" finished with status "complete"
    `<br>

    After waiting about 60 seconds, check the status of the "chat-app" job with this command on the "Server" tab:
    ```
    nomad job status chat-app
    ```

    This should indicate that the job is running and have an Allocations section at the bottom that looks like this:<br>
    `
    Allocations
    ID        Node ID   Task Group  Version  Desired  Status   Created    Modified
    14a5cc97  6777b2ca  chat-app    0        run      running  1m11s ago  24s ago
    2f792469  4b19ed9a  chat-app    0        run      running  1m11s ago  16s ago
    5d57baab  9afadcdd  chat-app    0        run      running  1m11s ago  16s ago
    `<br>

    Note that the Node IDs of the 3 allocations are all different. In other words, Nomad scheduled one instance of the chat-app task to each Nomad client because of the `spread` stanza we included in the "chat-app.nomad" job specification.

    You can also check out the "chat-app" job in the Nomad UI. Within 60 seconds of running the job, all 3 allocations should be healthy.

    You can also verify that the Nomad task groups were automatically registered as Consul services by looking at the "Services" tab of the Consul UI. Note the sidecar proxy services created by Nomad's integration with Consul Connect. The services all include node checks and service checks.

    ## Run the nginx.nomad Job
    Next, inspect the "nginx.nomad" job specification file on the "Jobs" tab. This job deploys [nginx](https://nginx.org) as a system job on all the Nomad clients from a Docker image. Since it is a system job, no "count" is specified for the "nginx" task group.

    nginx will route requests to the "chat-app" Consul service registered by the "chat-app" job; this is done by the "template" stanza of the job which generates the nginx configuration file, "load-balancer.conf", based on the current instances of the "chat-app" service. (This explains why we run the "nginx.nomad" job after the "chat-app.nomad" job.)

    The "ip_hash" instruction in the "load-balancer.conf" configuration file is included to make the nginx sessions of the chat app sticky. This is required to avoid HTTP 400 errors that would otherwise occur when multiple requests are sent during the lifetime of a socket used by the socket.io framework. (See this [doc](https://socket.io/docs/using-multiple-nodes) for details.)

    Run the "nginx.nomad" job on the "Server" tab with this command:
    ```
    nomad job run nginx.nomad
    ```
    This should return something like this:<br>
    `
    ==> Monitoring evaluation "32fbe07d"
    Evaluation triggered by job "nginx"
    Allocation "38f8f053" created: node "9afadcdd", group "nginx"
    Allocation "58e6b570" created: node "6777b2ca", group "nginx"
    Allocation "683d7322" created: node "4b19ed9a", group "nginx"
    Evaluation status changed: "pending" -> "complete"
    ==> Evaluation "32fbe07d" finished with status "complete"
    `<br>

    You can now access the chat-app UI on the "Chat 1", "Chat 2", and "Chat 3" tabs. Note that all instances of the chat app currently have a light background.

    Since we are using nginx, the 3 Chat tabs are not actually pinned to specific Nomad clients. In fact, all of them are pointing at the nginx load balancer itself on the corresponding Nomad client. Each time you click the refresh button for any of the Chat tabs, you will be randomly connected to an instance of the chat app on any of the 3 Nomad clients in a new session. As mentioned above, however, each session of the chat app between refreshes is sticky.

    If you type messages in the Chat tabs, they will usually show up in the other tab. However, you might sometimes need to click the Instruqt refresh button to the right of all the tabs while one of the Chat tabs is selected in order to force it to reconnect to the database. This is an issue with the chat app itself rather than with Nomad.

    In the next challenge, you will update the Chat app to use a dark background using Nomad's rolling update strategy.
  notes:
  - type: text
    contents: |-
      In this challenge, you will deploy the "mongodb", "chat-app", and "nginx" jobs.

      In the next challenge, you will update the Chat app using the rolling update strategy.
  tabs:
  - title: Jobs
    type: code
    hostname: nomad-server-1
    path: /root/nomad/jobs/
  - title: Server
    type: terminal
    hostname: nomad-server-1
  - title: Consul UI
    type: service
    hostname: nomad-server-1
    port: 8500
  - title: Nomad UI
    type: service
    hostname: nomad-server-1
    port: 4646
  - title: nginx UI
    type: service
    hostname: nomad-server-1
    port: 8080
  - title: Chat 1
    type: service
    hostname: nomad-client-1
    port: 8080
  - title: Chat 2
    type: service
    hostname: nomad-client-2
    port: 8080
  - title: Chat 3
    type: service
    hostname: nomad-client-3
    port: 8080
  difficulty: basic
  timelimit: 3600
checksum: "13992001899526899210"
